{
	// Use IntelliSense to learn about possible attributes.
	// Hover to view descriptions of existing attributes.
	// For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
	"version": "0.2.0",
	"configurations": [
		{
			"name": "evaluate for each instances",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true,
			"args": [
				"prompt_way=own",
				"batch_size=4",
				"offset=0",
				"data_dir=/fs/ess/PAA0201/lzy37ld/why_attack/data/results_n_steps_1000_llama2-chat",
				"data_prefix=individual_behaviors_llama2-chat_gcg_offset${offset}.json",
				"target_lm=llama2-chat",
				"adv_prompt_steps_per_instances=1000"
			]
		},
		{
			"name": "evaluate prompter",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true,
			"env": {
				"CUDA_DEVICE_ORDER": "PCI_BUS_ID",
				"CUDA_VISIBLE_DEVICES": "1,2"
			}
		},
		{
			"name": "test saving ckpt",
			"type": "python",
			"request": "launch",
			"program": "/home/liao.629/miniconda3/envs/attack/bin/torchrun",
			"console": "integratedTerminal",
			"justMyCode": false,
			"env": {
				"CUDA_DEVICE_ORDER": "PCI_BUS_ID",
				"CUDA_VISIBLE_DEVICES": "1,2"
			},
			"args": [
				"--nproc_per_node=2",
				"--master_port=1234",
				"train_prompter.py",
				"--model_name_or_path",
				// "meta-llama/Llama-2-7b-hf",
				"gpt2",
				"--data_path",
				"data/vicuna_process_100.json",
				"--bf16",
				"True",
				"--output_dir",
				"vicuna_ckpt_test",
				"--num_train_epochs",
				"3",
				"--per_device_train_batch_size",
				"1",
				"--per_device_eval_batch_size",
				"4",
				"--gradient_accumulation_steps",
				"2",
				"--save_total_limit",
				"2",
				"--evaluation_strategy",
				"no",
				"--save_strategy",
				"steps",
				"--max_steps",
				"2",
				"--save_steps",
				"1",
				"--learning_rate",
				"5e-5",
				"--weight_decay",
				"0.",
				"--warmup_ratio",
				"0.03",
				"--lr_scheduler_type",
				"cosine",
				"--logging_steps",
				"1",
				"--fsdp",
				"full_shard auto_wrap",
				// "--fsdp_transformer_layer_cls_to_wrap",
				// "LlamaDecoderLayer",
				"--tf32",
				"True",
				"--report_to",
				"none",
				"--prompt_type",
				"q_r_p",
				"--train_ratio",
				"0.8"
			]
		},
		{
			"name": "train prompter.py",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true,
			"args": [
				"--model_name_or_path",
				"meta-llama/Llama-2-7b-hf",
				"--data_path",
				"data/vicuna_process_100.json",
				"--bf16",
				"True",
				"--output_dir",
				"prompter_ckpt",
				"--num_train_epochs",
				"3",
				"--per_device_train_batch_size",
				"4",
				"--per_device_eval_batch_size",
				"4",
				"--gradient_accumulation_steps",
				"8",
				"--evaluation_strategy",
				"no",
				"--save_strategy",
				"steps",
				"--save_steps",
				"2000",
				"--save_total_limit",
				"1",
				"--learning_rate",
				"2e-5",
				"--weight_decay",
				"0.",
				"--warmup_ratio",
				"0.03",
				"--lr_scheduler_type",
				"cosine",
				"--logging_steps",
				"1",
				"--fsdp",
				"full_shard auto_wrap",
				"--fsdp_transformer_layer_cls_to_wrap",
				"LlamaDecoderLayer",
				"--tf32",
				"True"
			]
		},
		{
			"name": "evaluate for test.py",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true,
			"args": [
				"offset=510"
			]
		},
		{
			"name": "check tokens.py",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": false,
			"env": {
				"CUDA_DEVICE_ORDER": "PCI_BUS_ID",
				"CUDA_VISIBLE_DEVICES": "0",
			},
		},
		{
			"name": "Python: Current File",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true
		}
	]
}