{
	// Use IntelliSense to learn about possible attributes.
	// Hover to view descriptions of existing attributes.
	// For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
	"version": "0.2.0",
	"configurations": [
		{
			"name": "trainer add ppl loss",
			"type": "python",
			"request": "launch",
			"program": "/home/liao.629/miniconda3/envs/attack/bin/accelerate",
			"console": "integratedTerminal",
			"justMyCode": false,
			"args": [
				"launch",
				"--num_processes",
				"2",
				"--main_process_port",
				"1239",
				"--gpu_ids",
				"3,4",
				"train_prompter.py",
				"--model_name_or_path",
				// "meta-llama/Llama-2-7b-hf",
				"gpt2",
				"--sampled_queries",
				"data/success_JB_victimmodel=llama2-7b-chat_sampleway=step_nsample=200.json",
				"--split_path",
				"data/train_val_test.json",
				"--bf16",
				"True",
				"--output_dir",
				"./test_ckpt",
				"--num_train_epochs",
				"5",
				"--per_device_train_batch_size",
				"1",
				"--per_device_eval_batch_size",
				"4",
				"--gradient_accumulation_steps",
				"2",
				"--evaluation_strategy",
				"no",
				"--save_strategy",
				"steps",
				"--save_steps",
				"5000",
				"--learning_rate",
				"5e-5",
				"--weight_decay",
				"0.",
				"--warmup_ratio",
				"0.03",
				"--lr_scheduler_type",
				"cosine",
				"--logging_steps",
				"1",
				"--fsdp",
				"full_shard auto_wrap",
				// "--fsdp_transformer_layer_cls_to_wrap",
				// "'LlamaDecoderLayer'",
				"--tf32",
				"True",
				"--prompt_type",
				"q_p",
				"--ppl_loss"
			]
		},
		{
			"name": "evaluate prompter",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": false,
			"args": [
				"target_lm=llama2-chat",
				"prompter_lm.model_name='/fs/ess/PAA0201/lzy37ld/why_attack/ckpt/prompter_victim=llama2-7b-chat_prompt_type=q_p_model_name=llama2-base_sample_way_and_n_sample=random_nsample=200_epoch_5/checkpoint-25000'",
				"prompter_lm.show_name=debug_mode",
				"data_args.split=val",
				"ppl=false",
				"s_p_t_dir=./debug_mode",
				"target_lm.generation_configs.max_new_tokens=100",
				"generation_configs@prompter_lm=top_p",
				"prompter_lm.generation_configs.num_return_sequences=50"
			]
		},
		{
			"name": "filter generated data",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true,
			"args": [
				"evaluated_data_path_template='/users/PAA0201/lzy37ld/why_attack_lzy/s_p_t_evaluate/llama2-7b-chat|max_new_tokens_60/{offset}|promptway_own|targetlm_do_sample_False|append_label_length_-1.jsonl'",
				"evaluated_model=llama2-7b-chat",
				"n_sample=200",
				"sample_way=step"
			]
		},
		{
			"name": "evaluate for each instances",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true,
			"args": [
				"prompt_way=own",
				"batch_size=4",
				"offset=0",
				"data_dir=/fs/ess/PAA0201/lzy37ld/why_attack_lzy/data/results_n_steps_1000_llama2-chat",
				"data_prefix=individual_behaviors_llama2-chat_gcg_offset${offset}.json",
				"target_lm=llama2-chat",
				"adv_prompt_steps_per_instances=1000"
			]
		},
		{
			"name": "evaluate prompter",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true,
			"env": {
				"CUDA_DEVICE_ORDER": "PCI_BUS_ID",
				"CUDA_VISIBLE_DEVICES": "1,2"
			}
		},
		{
			"name": "test saving ckpt",
			"type": "python",
			"request": "launch",
			"program": "/home/liao.629/miniconda3/envs/attack/bin/torchrun",
			"console": "integratedTerminal",
			"justMyCode": false,
			"env": {
				"CUDA_DEVICE_ORDER": "PCI_BUS_ID",
				"CUDA_VISIBLE_DEVICES": "1,2"
			},
			"args": [
				"--nproc_per_node=2",
				"--master_port=1234",
				"train_prompter.py",
				"--model_name_or_path",
				// "meta-llama/Llama-2-7b-hf",
				"gpt2",
				"--data_path",
				"data/vicuna_process_100.json",
				"--bf16",
				"True",
				"--output_dir",
				"vicuna_ckpt_test",
				"--num_train_epochs",
				"3",
				"--per_device_train_batch_size",
				"1",
				"--per_device_eval_batch_size",
				"4",
				"--gradient_accumulation_steps",
				"2",
				"--save_total_limit",
				"2",
				"--evaluation_strategy",
				"no",
				"--save_strategy",
				"steps",
				"--max_steps",
				"2",
				"--save_steps",
				"1",
				"--learning_rate",
				"5e-5",
				"--weight_decay",
				"0.",
				"--warmup_ratio",
				"0.03",
				"--lr_scheduler_type",
				"cosine",
				"--logging_steps",
				"1",
				"--fsdp",
				"full_shard auto_wrap",
				// "--fsdp_transformer_layer_cls_to_wrap",
				// "LlamaDecoderLayer",
				"--tf32",
				"True",
				"--report_to",
				"none",
				"--prompt_type",
				"q_r_p",
				"--train_ratio",
				"0.8"
			]
		},
		{
			"name": "train prompter.py",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true,
			"args": [
				"--model_name_or_path",
				"meta-llama/Llama-2-7b-hf",
				"--data_path",
				"data/vicuna_process_100.json",
				"--bf16",
				"True",
				"--output_dir",
				"prompter_ckpt",
				"--num_train_epochs",
				"3",
				"--per_device_train_batch_size",
				"4",
				"--per_device_eval_batch_size",
				"4",
				"--gradient_accumulation_steps",
				"8",
				"--evaluation_strategy",
				"no",
				"--save_strategy",
				"steps",
				"--save_steps",
				"2000",
				"--save_total_limit",
				"1",
				"--learning_rate",
				"2e-5",
				"--weight_decay",
				"0.",
				"--warmup_ratio",
				"0.03",
				"--lr_scheduler_type",
				"cosine",
				"--logging_steps",
				"1",
				"--fsdp",
				"full_shard auto_wrap",
				"--fsdp_transformer_layer_cls_to_wrap",
				"LlamaDecoderLayer",
				"--tf32",
				"True"
			]
		},
		{
			"name": "evaluate for test.py",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true,
			"args": [
				"offset=510"
			]
		},
		{
			"name": "check tokens.py",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": false,
			"env": {
				"CUDA_DEVICE_ORDER": "PCI_BUS_ID",
				"CUDA_VISIBLE_DEVICES": "0",
			},
		},
		{
			"name": "Python: Current File",
			"type": "python",
			"request": "launch",
			"program": "${file}",
			"console": "integratedTerminal",
			"justMyCode": true
		}
	]
}