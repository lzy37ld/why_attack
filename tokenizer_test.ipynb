{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message= (\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "        \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "        \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \"\n",
    "        \"If you don't know the answer to a question, please don't share false information.\")\n",
    "prompt = \"Hello, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] ICL1\n",
      "ICL2\n",
      "ICL3 [/INST]\n",
      "**************************************************\n",
      "<s>[INST] ICL1\n",
      "ICL2\n",
      "ICL3 [/INST]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "chat = [\n",
    "# {\n",
    "#     \"role\": \"system\",\n",
    "#     \"content\": (\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "#     \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "#     \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "#     \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \"\n",
    "#     \"If you don't know the answer to a question, please don't share false information.\")\n",
    "# },\n",
    "{\"role\": \"user\", \"content\": \"ICL1\\nICL2\\nICL3\"},\n",
    "]\n",
    "\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "print(x)\n",
    "print(\"*\"*50)\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "print(x)\n",
    "# print(tokenizer.decode(tokenizer(x).input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Hello', ',', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "# no system message\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "\n",
    "chat = [\n",
    "{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "]\n",
    "\n",
    "x = tokenizer(\"Hello, how are you?\")\n",
    "print(tokenizer.batch_decode(x.input_ids))\n",
    "# print(\"*\"*50)\n",
    "# x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "# print(x)\n",
    "# print(tokenizer.decode(tokenizer(x).input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>User: Hello, how are you?\n",
      "\n",
      "\n",
      "**************************************************\n",
      "<｜begin▁of▁sentence｜>User: Hello, how are you?\n",
      "\n",
      "Assistant:\n",
      "1\n",
      "<｜begin▁of▁sentence｜>I am good\n"
     ]
    }
   ],
   "source": [
    "# no system messgae...\n",
    "\n",
    "chat = [\n",
    "{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "]\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-chat\")\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "print(x)\n",
    "print(\"*\"*50)\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "print(x)\n",
    "print(1)\n",
    "\n",
    "x = \"I am good\"\n",
    "print(tokenizer.decode(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>User: Hello, how are you?\n",
      "\n",
      "Assistant:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "p = \"<｜begin▁of▁sentence｜>User: {prompt}\\n\\nAssistant:\"\n",
    "print(p.format(prompt = prompt))\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "\n",
      "**************************************************\n",
      "<|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "]\n",
    "\n",
    "\n",
    "from transformers import  AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-6B-Chat\")\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "print(x)\n",
    "print(\"*\"*50)\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "print(x)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am good\n",
      "[616, 1064, 1226]\n",
      "<|startoftext|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-6B-Chat\")\n",
    "x = \"I am good\"\n",
    "print(tokenizer.decode(tokenizer.encode(x)))\n",
    "print(tokenizer(x).input_ids)\n",
    "print(tokenizer.bos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "p = \"<|im_start|>system\\n{system_message}\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "print(p.format(system_message = system_message, prompt = prompt))\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 851, 349, 1179]\n",
      "<s> This is good\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "x = \"<s>This is good\"\n",
    "print(tokenizer.encode(x,add_special_tokens=False))\n",
    "print(tokenizer.decode(tokenizer.encode(x,add_special_tokens= False)))\n",
    "# # \n",
    "# # \n",
    "# # base model dont have special format\n",
    "# # \n",
    "# # \n",
    "# chat = [\n",
    "# {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "# {\"role\": \"assistant\", \"content\": \"Good\"}\n",
    "# ]\n",
    "\n",
    "# x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "# print(x)\n",
    "# print(\"*\"*50)\n",
    "# x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "# print(x)\n",
    "# # print(tokenizer.decode(tokenizer(x).input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TemplateError",
     "evalue": "Conversation roles must alternate user/assistant/user/assistant/...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTemplateError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m chat \u001b[39m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m \t{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWhat is your favourite condiment?\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      5\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWhat is your favourite condiment?\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      6\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWell, I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm cooking up in the kitchen!\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDo you have mayonnaise recipes?\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m x \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mapply_chat_template(chat, tokenize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,add_generation_prompt \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m50\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1743\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[39m# Compilation function uses a cache to avoid recompiling the same template\u001b[39;00m\n\u001b[1;32m   1741\u001b[0m compiled_template \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compile_jinja_template(chat_template)\n\u001b[0;32m-> 1743\u001b[0m rendered \u001b[39m=\u001b[39m compiled_template\u001b[39m.\u001b[39;49mrender(\n\u001b[1;32m   1744\u001b[0m     messages\u001b[39m=\u001b[39;49mconversation, add_generation_prompt\u001b[39m=\u001b[39;49madd_generation_prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspecial_tokens_map\n\u001b[1;32m   1745\u001b[0m )\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m padding \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m   1748\u001b[0m     padding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# There's only one sequence here, so \"longest\" makes no sense\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/jinja2/environment.py:1301\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment\u001b[39m.\u001b[39mconcat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_render_func(ctx))  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvironment\u001b[39m.\u001b[39;49mhandle_exception()\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/jinja2/environment.py:936\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdebug\u001b[39;00m \u001b[39mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 936\u001b[0m \u001b[39mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[39m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:1\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/jinja2/sandbox.py:393\u001b[0m, in \u001b[0;36mSandboxedEnvironment.call\u001b[0;34m(_SandboxedEnvironment__self, _SandboxedEnvironment__context, _SandboxedEnvironment__obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m __self\u001b[39m.\u001b[39mis_safe_callable(__obj):\n\u001b[1;32m    392\u001b[0m     \u001b[39mraise\u001b[39;00m SecurityError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m__obj\u001b[39m!r}\u001b[39;00m\u001b[39m is not safely callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[39mreturn\u001b[39;00m __context\u001b[39m.\u001b[39;49mcall(__obj, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1771\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._compile_jinja_template.<locals>.raise_exception\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_exception\u001b[39m(message):\n\u001b[0;32m-> 1771\u001b[0m     \u001b[39mraise\u001b[39;00m TemplateError(message)\n",
      "\u001b[0;31mTemplateError\u001b[0m: Conversation roles must alternate user/assistant/user/assistant/..."
     ]
    }
   ],
   "source": [
    "name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "chat = [\n",
    "\t{\"role\": \"system\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "print(x)\n",
    "print(\"*\"*50)\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "print(x)\n",
    "# print(tokenizer.decode(tokenizer(x).input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> system_messageUSER: input prompt ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "# vicuna chat template in HF is wrong\n",
    "\n",
    "name = \"lmsys/vicuna-7b-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "\n",
    "x = \"system_messageUSER: input prompt ASSISTANT:\"\n",
    "print(tokenizer.decode(tokenizer.encode(x,add_special_tokens= True)))\n",
    "\n",
    "# chat = [\n",
    "# \t{\"role\": \"system\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "# x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "# print(x)\n",
    "# print(\"*\"*50)\n",
    "# x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "# print(x)\n",
    "# # print(tokenizer.decode(tokenizer(x).input_ids))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
