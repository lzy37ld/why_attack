{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message= (\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "        \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "        \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \"\n",
    "        \"If you don't know the answer to a question, please don't share false information.\")\n",
    "prompt = \"Hello, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] ICL1\n",
      "ICL2\n",
      "ICL3 [/INST]\n",
      "**************************************************\n",
      "<s>[INST] ICL1\n",
      "ICL2\n",
      "ICL3 [/INST]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "chat = [\n",
    "# {\n",
    "#     \"role\": \"system\",\n",
    "#     \"content\": (\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "#     \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
    "#     \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "#     \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \"\n",
    "#     \"If you don't know the answer to a question, please don't share false information.\")\n",
    "# },\n",
    "{\"role\": \"user\", \"content\": \"ICL1\\nICL2\\nICL3\"},\n",
    "]\n",
    "\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "print(x)\n",
    "print(\"*\"*50)\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "print(x)\n",
    "# print(tokenizer.decode(tokenizer(x).input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Hello', ',', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "# no system message\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "\n",
    "chat = [\n",
    "{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "]\n",
    "\n",
    "x = tokenizer(\"Hello, how are you?\")\n",
    "print(tokenizer.batch_decode(x.input_ids))\n",
    "# print(\"*\"*50)\n",
    "# x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "# print(x)\n",
    "# print(tokenizer.decode(tokenizer(x).input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>User: Hello, how are you?\n",
      "\n",
      "\n",
      "**************************************************\n",
      "<｜begin▁of▁sentence｜>User: Hello, how are you?\n",
      "\n",
      "Assistant:\n",
      "1\n",
      "<｜begin▁of▁sentence｜>I am good\n"
     ]
    }
   ],
   "source": [
    "# no system messgae...\n",
    "\n",
    "chat = [\n",
    "{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "]\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-chat\")\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "print(x)\n",
    "print(\"*\"*50)\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "print(x)\n",
    "print(1)\n",
    "\n",
    "x = \"I am good\"\n",
    "print(tokenizer.decode(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>User: Hello, how are you?\n",
      "\n",
      "Assistant:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "p = \"<｜begin▁of▁sentence｜>User: {prompt}\\n\\nAssistant:\"\n",
    "print(p.format(prompt = prompt))\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "\n",
      "**************************************************\n",
      "<|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "]\n",
    "\n",
    "\n",
    "from transformers import  AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-6B-Chat\")\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "print(x)\n",
    "print(\"*\"*50)\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "print(x)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am good\n",
      "[616, 1064, 1226]\n",
      "<|startoftext|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-6B-Chat\")\n",
    "x = \"I am good\"\n",
    "print(tokenizer.decode(tokenizer.encode(x)))\n",
    "print(tokenizer(x).input_ids)\n",
    "print(tokenizer.bos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "p = \"<|im_start|>system\\n{system_message}\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "print(p.format(system_message = system_message, prompt = prompt))\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 851, 349, 1179]\n",
      "<s> This is good\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "x = \"<s>This is good\"\n",
    "print(tokenizer.encode(x,add_special_tokens=False))\n",
    "print(tokenizer.decode(tokenizer.encode(x,add_special_tokens= False)))\n",
    "# # \n",
    "# # \n",
    "# # base model dont have special format\n",
    "# # \n",
    "# # \n",
    "# chat = [\n",
    "# {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "# {\"role\": \"assistant\", \"content\": \"Good\"}\n",
    "# ]\n",
    "\n",
    "# x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "# print(x)\n",
    "# print(\"*\"*50)\n",
    "# x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "# print(x)\n",
    "# # print(tokenizer.decode(tokenizer(x).input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TemplateError",
     "evalue": "Conversation roles must alternate user/assistant/user/assistant/...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTemplateError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m chat \u001b[39m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m \t{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWhat is your favourite condiment?\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      5\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWhat is your favourite condiment?\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      6\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWell, I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm cooking up in the kitchen!\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDo you have mayonnaise recipes?\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m x \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mapply_chat_template(chat, tokenize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,add_generation_prompt \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m50\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1743\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[39m# Compilation function uses a cache to avoid recompiling the same template\u001b[39;00m\n\u001b[1;32m   1741\u001b[0m compiled_template \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compile_jinja_template(chat_template)\n\u001b[0;32m-> 1743\u001b[0m rendered \u001b[39m=\u001b[39m compiled_template\u001b[39m.\u001b[39;49mrender(\n\u001b[1;32m   1744\u001b[0m     messages\u001b[39m=\u001b[39;49mconversation, add_generation_prompt\u001b[39m=\u001b[39;49madd_generation_prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspecial_tokens_map\n\u001b[1;32m   1745\u001b[0m )\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m padding \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m   1748\u001b[0m     padding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# There's only one sequence here, so \"longest\" makes no sense\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/jinja2/environment.py:1301\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment\u001b[39m.\u001b[39mconcat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_render_func(ctx))  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvironment\u001b[39m.\u001b[39;49mhandle_exception()\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/jinja2/environment.py:936\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdebug\u001b[39;00m \u001b[39mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 936\u001b[0m \u001b[39mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[39m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:1\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/jinja2/sandbox.py:393\u001b[0m, in \u001b[0;36mSandboxedEnvironment.call\u001b[0;34m(_SandboxedEnvironment__self, _SandboxedEnvironment__context, _SandboxedEnvironment__obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m __self\u001b[39m.\u001b[39mis_safe_callable(__obj):\n\u001b[1;32m    392\u001b[0m     \u001b[39mraise\u001b[39;00m SecurityError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m__obj\u001b[39m!r}\u001b[39;00m\u001b[39m is not safely callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[39mreturn\u001b[39;00m __context\u001b[39m.\u001b[39;49mcall(__obj, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1771\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._compile_jinja_template.<locals>.raise_exception\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_exception\u001b[39m(message):\n\u001b[0;32m-> 1771\u001b[0m     \u001b[39mraise\u001b[39;00m TemplateError(message)\n",
      "\u001b[0;31mTemplateError\u001b[0m: Conversation roles must alternate user/assistant/user/assistant/..."
     ]
    }
   ],
   "source": [
    "name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "chat = [\n",
    "\t{\"role\": \"system\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "print(x)\n",
    "print(\"*\"*50)\n",
    "x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "print(x)\n",
    "# print(tokenizer.decode(tokenizer(x).input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> system_messageUSER: input prompt ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "# vicuna chat template in HF is wrong\n",
    "\n",
    "name = \"lmsys/vicuna-7b-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "\n",
    "x = \"system_messageUSER: input prompt ASSISTANT:\"\n",
    "print(tokenizer.decode(tokenizer.encode(x,add_special_tokens= True)))\n",
    "\n",
    "# chat = [\n",
    "# \t{\"role\": \"system\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "# x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = False)\n",
    "# print(x)\n",
    "# print(\"*\"*50)\n",
    "# x = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt = True)\n",
    "# print(x)\n",
    "# # print(tokenizer.decode(tokenizer(x).input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '/local/scratch/liao.629/why_attack/ckpt/prompter_vicuna_ckpt_gpt2_q_r_p/'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/transformers/src/transformers/utils/hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    390\u001b[0m         path_or_repo_id,\n\u001b[1;32m    391\u001b[0m         filename,\n\u001b[1;32m    392\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    393\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    394\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    395\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    396\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    397\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    398\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    399\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    400\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    401\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    112\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/attack/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/local/scratch/liao.629/why_attack/ckpt/prompter_vicuna_ckpt_gpt2_q_r_p/'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/local/scratch/liao.629/why_attack/ckpt/prompter_vicuna_ckpt_gpt2_q_r_p/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m# m = AutoModelForCausalLM.from_pretrained(name)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m t \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(name)\n",
      "File \u001b[0;32m~/transformers/src/transformers/models/auto/tokenization_auto.py:738\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    737\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    739\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    740\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/transformers/src/transformers/models/auto/tokenization_auto.py:570\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     token \u001b[39m=\u001b[39m use_auth_token\n\u001b[1;32m    569\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 570\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    571\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    572\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    573\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    574\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    575\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    576\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    577\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    578\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    579\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    580\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    581\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    582\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    583\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    584\u001b[0m )\n\u001b[1;32m    585\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/transformers/src/transformers/utils/hub.py:454\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThere was a specific connection error when trying to load \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    453\u001b[0m \u001b[39mexcept\u001b[39;00m HFValidationError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 454\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    455\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncorrect path_or_model_id: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[39mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/local/scratch/liao.629/why_attack/ckpt/prompter_vicuna_ckpt_gpt2_q_r_p/'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "name = \"/local/scratch/liao.629/why_attack/ckpt/prompter_vicuna_ckpt_gpt2_q_r_p/\"\n",
    "# m = AutoModelForCausalLM.from_pretrained(name)\n",
    "t = AutoTokenizer.from_pretrained(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
